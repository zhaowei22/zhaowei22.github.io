<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[python time 笔记]]></title>
    <url>%2F2653527150%2F</url>
    <content type="text"><![CDATA[在 python 中，操作时间 和 日期的模块有 time 和 datetime，calendar time 简单的时间输出或转换 datetime 简单和复杂的时间格式化 calendar 操作日历的模块 相关概念 时间戳： 是指格林威治时间1970年01月01日00时00分00秒(北京时间1970年01月01日08时00分00秒)起至现在的总秒数。通俗的讲， 时间戳是一份能够表示一份数据在一个特定时间点已经存在的完整的可验证的数据。 它的提出主要是为用户提供一份电子证据， 以证明用户的某些数据的产生时间。 在实际应用上， 它可以使用在包括电子商务、 金融活动的各个方面， 尤其可以用来支撑公开密钥基础设施的 “不可否认” 服务。时间戳单位最适于做日期运算。但是1970年之前的日期就无法以此表示了。太遥远的日期也不行，UNIX和Windows只支持到2038年。 时间元组： 用元组形式表示一个时间点 属性 注释 取值 tm_year 4位数年 4位数，2019 tm_mon 月 1 到 12 tm_mday 日 1到31 tm_hour 时 0到23 tm_min 分 0到59 tm_sec 秒 0到61 (60或61 是闰秒) tm_wday 一周的第几日 0到6 (0是周一) tm_yday 一年的第几日 1到366 (儒略历) tm_isdst 夏令时 -1, 0, 1, -1是决定是否为夏令时的旗帜 python 格式化时间符号 12345678910111213141516171819202122%y # 两位数的年份表示（00-99）%Y # 四位数的年份表示（000-9999）%m # 月份（01-12）%d # 月内中的一天（0-31）%H # 24小时制小时数（0-23）%I # 12小时制小时数（01-12）%M # 分钟数（00=59）%S # 秒（00-59）%a # 本地简化星期名称%A # 本地完整星期名称%b # 本地简化的月份名称%B # 本地完整的月份名称%c # 本地相应的日期表示和时间表示%j # 年内的一天（001-366）%p # 本地A.M.或P.M.的等价符%U # 一年中的星期数（00-53）星期天为星期的开始%w # 星期（0-6），星期天为星期的开始%W # 一年中的星期数（00-53）星期一为星期的开始%x # 本地相应的日期表示%X # 本地相应的时间表示%Z # 当前时区的名称%% # %号本身 time 模块12345678910111213141516171819202122232425262728293031323334353637383940414243# -*- coding: UTF-8 -*-import time# 获取当前时间戳# 1558162277.7014651timestamp = time.time()# 获取时间元组；可获取指定时间# time.struct_time(tm_year=2019, tm_mon=5, tm_mday=18, tm_hour=14, tm_min=51, tm_sec=17, tm_wday=5, tm_yday=138, tm_isdst=0)localtime = time.localtime(timestamp)# 将时间元组格式化为可读时间# 'Sat May 18 14:51:17 2019'localtime = time.asctime(localtime)# 使用 strftime 方法来格式化日期# '2019-05-18 15:06:05'time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())# Sat May 18 15:07:06 2019'time.strftime("%a %b %d %H:%M:%S %Y", time.localtime())dt = "2019-04-27 20:22:45"# 将字符串时间转换成时间数组；# time.struct_time(tm_year=2019, tm_mon=4, tm_mday=27, tm_hour=20, tm_min=22, tm_sec=45, tm_wday=5, tm_yday=117, tm_isdst=-1)time_tuple = time.strptime(dt, "%Y-%m-%d %H:%M:%S")# 将时间元组转换成时间戳# 556367765.0 时间间隔是以秒为单位的浮点小数timestamp = time.mktime(time_tuple)# 推迟调用线程的运行time.sleep(secs)# 属性# 当地时区（未启动夏令时）距离格林威治的偏移秒数（&gt;0，美洲;&lt;=0大部分欧洲，亚洲，非洲）# -28800time.timezone# 属性time.tzname包含一对根据情况的不同而不同的字符串，分别是带夏令时的本地时区名称，和不带的# ('CST', 'CST')time.tzname datetime 模块datetime 模块包含的类 日期，属性: year, month, day12345678910111213141516171819202122232425```time``` 时间，属性: hour, minute, second, microsecond, tzinfo```datetime``` 日期和时间的组合```pythonfrom datetime import date, time, datetime# datetime(year, month, day[, hour[, minute[, second[, microsecond[, tzinfo]]]]])date.today()# datetime.date(2019, 5, 25)datetime.now()# datetime.datetime(2019, 5, 25, 22, 15, 49, 423978)datetime.utcnow()# datetime.datetime(2019, 5, 27, 14, 16, 28, 998506)datetime.strptime(&apos;2019-5-25 20:25&apos;,&apos;%Y-%m-%d %H:%M&apos;)# datetime.datetime(2019, 5, 25, 20, 25)datetime.strftime(&quot;%Y-%m-%d %H:%M:%S&quot;)# &apos;2019-05-25 22:19:28&apos;a = datetime.datetime.now()datetime.datetime.combine(a.date(),a.time())# 将date 和 time 合并为 datetime 日期/时间差123456789101112131415161718```pythonfrom datetime import timedelta# timedelta([days[, seconds[, microseconds[, milliseconds[, minutes[, hours[, weeks]]]]]]])td = timedelta(days=30)td.days()# 30td.total_seconds()# 2592000.0 == 30 * 3600 * 24str(td)# &apos;30 days, 0:00:00&apos;# timedelta 支持数学运算# + - * // abs 正 负 时区信息123456789101112131415161718192021222324252627282930313233343536## calendar 模块```python#!/usr/bin/python# -*- coding: UTF-8 -*-import calendar cal = calendar.month(2019, 5)print(&quot;以下输出2019年5月份的日历:&quot;)print(cal)# May 2019# Mo Tu We Th Fr Sa Su# 1 2 3 4 5# 6 7 8 9 10 11 12# 13 14 15 16 17 18 19# 20 21 22 23 24 25 26# 27 28 29 30 31timegm(tuple)# 将datetime 元组格式转换成 时间戳格式calendar.isleap(year)# 判断是否为闰年，返回bool值calendar.leapdays(y1, y2)# 返回y1 至 y2(不包括) 年份之间包含的闰年数calendar.firstweekday()# 查询当前设置的一周的第一天calendar.weekday(year, month, day)# 返回给定 年月日 是星期几]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CDN详解]]></title>
    <url>%2F3523045194%2F</url>
    <content type="text"><![CDATA[概念 CDN 的全称是Content Delivery Network，即内容分发网络。CDN是构建在网络之上的内容分发网络，依靠部署在各地的边缘服务器，通过中心平台的负载均衡、内容分发、调度等功能模块，使用户就近获取所需内容，降低网络拥塞，提高用户访问响应速度和命中率。CDN的关键技术主要有内容存储和分发技术。 —百度百科 基本思路 基本思路是尽可能避开互联网上有可能影响数据传输速度和稳定性的瓶颈和环节，使内容传输的更快、更稳定。通过在网络各处放置节点服务器所构成的在现有的互联网基础之上的一层智能虚拟网络，CDN系统能够实时地根据网络流量和各节点的连接、负载状况以及到用户的距离和响应时间等综合信息将用户的请求重新导向离用户最近的服务节点上。其目的是使用户可就近取得所需内容，解决 Internet网络拥挤的状况，提高用户访问网站的响应速度。 CDN 主要是为了解决网络拥挤的状况，避免网络不佳带来的延迟，同时也解决了不同运营商之间的带宽限制，提高用户的访问速度 关键技术 内容发布：它借助于建立索引、缓存、流分裂、组播（Multicast）等技术，将内容发布或投递到距离用户最近的远程服务点（POP）处 内容路由：它是整体性的网络负载均衡技术，通过内容路由器中的重定向（DNS）机制，在多个远程POP上均衡用户的请求，以使用户请求得到最近内容源的响应 内容交换：它根据内容的可用性、服务器的可用性以及用户的背景，在POP的缓存服务器上，利用应用层交换、流分裂、重定向（ICP、WCCP）等技术，智能地平衡负载流量 性能管理：它通过内部和外部监控系统，获取网络部件的状况信息，测量内容发布的端到端性能（如包丢失、延时、平均带宽、启动时间、帧速率等），保证网络处于最佳的运行状态。 CDN的优势 CDN能几乎涵盖国内所有线路。而在可靠性上， CDN 在结构上实现了多点的冗余，即使某一个节点由于意外发生故障，对网站的访问能够被自动导向其他的健康节点进行响应。CDN能轻松实现网站的全国铺设，不必考虑服务器的投入与托管、不必考虑新增带宽的成本、不必考虑多台服务器的镜像同步、不必考虑更多的管理维护技术人员。 消除了不同运营商之间互联的瓶颈造成的影响，实现了跨运营商的网络加速，保证不同网络中的用户都能得到良好的访问质量。 广泛分布的CDN节点加上节点之间的智能冗余机制，可以有效地预防黑客入侵以及降低各种D.D.o.S攻击对网站的影响，同时保证较好的服务质量 。 CDN 基本工作流程正常用户请求流程 用户在自己的浏览器中输入要访问的网站域名。 浏览器向本地DNS服务器请求对该域名的解析。 本地DNS服务器中如果缓存有这个域名的解析结果，则直接响应用户的解析请求。 本地DNS服务器中如果没有关于这个域名的解析结果的缓存，则以递归方式向整个DNS系统请求解析，获得应答后将结果反馈给浏览器。 浏览器得到域名解析结果，就是该域名相应的服务设备的IP地址。 浏览器向服务器请求内容。 服务器将用户请求内容传送给浏览器。 添加CDN后 用户请求流程 当用户点击网站页面上的内容URL，经过本地DNS系统解析，DNS系统会最终将域名的解析权交给CNAME指向的CDN专用DNS服务器。 CDN的DNS服务器将CDN的全局负载均衡设备IP地址返回用户。 用户向CDN的全局负载均衡设备发起内容URL访问请求。 CDN全局负载均衡设备根据用户IP地址，以及用户请求的内容URL，选择一台用户所属区域的区域负载均衡设备，告诉用户向这台设备发起请求。 区域负载均衡设备会为用户选择一台合适的缓存服务器提供服务，选择的依据包括：根据用户IP地址，判断哪一台服务器距用户最近；根据用户所请求的URL中携带的内容名称，判断哪一台服务器上有用户所需内容；查询各个服务器当前的负载情况，判断哪一台服务器尚有服务能力。基于以上这些条件的综合分析之后，区域负载均衡设备会向全局负载均衡设备返回一台缓存服务器的IP地址。 全局负载均衡设备把服务器的IP地址返回给用户。 用户向缓存服务器发起请求，缓存服务器响应用户请求，将用户所需内容传送到用户终端。如果这台缓存服务器上并没有用户想要的内容，而区域均衡设备依然将它分配给了用户，那么这台服务器就要向它的上一级缓存服务器请求内容，直至追溯到网站的源服务器将内容拉到本地。 CDN 名词解释阿里云 CDN 名词解释 CDN 配置流程（阿里CDN配置流程） 开通CDN服务 添加加速域名 配置CNAME 参考链接：CDN的基本工作过程 百度知道 CDN词条 阿里CDN配置]]></content>
      <categories>
        <category>环境搭建</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python 代码规范PEP8]]></title>
    <url>%2F1884161696%2F</url>
    <content type="text"><![CDATA[缩进使用四个空格表示每个缩进级别。 123456789101112131415161718192021# 对齐foo = long_function_name(var_one, var_two, var_three, var_four) # 比之后的内容多一层缩进# 使用更多的缩进以和其他的代码单元区别开来def long_function_name( var_one, var_two, var_three, var_four): print(var_one)# 悬垂的缩进，多加一层foo = long_function_name( var_one, var_two, var_three, var_four)# 或者：foo = long_func_name( var_one, var_two, var_three, var_four)# 另外其实有一种可选情况，也就是悬垂缩进可以缩进不为4个空格，比如用两个 不规范示例 12345678910# 当不适用垂直对齐时，禁止在第一行使用参数# 换句话说，在垂直对齐时，才可在第一行使用参数foo = lone_func_name(var_one, var_two, var_three, var_four)# 当缩进不足以区分代码结构时，增加一个缩进级别def long_func_name( var_one, var_two, var_three var_four): print(var_one) 最大长度所有行的最大长度均为79个字符 对于文档字符串或者注释，最长72字符 1234# 当超出最大长度时，使用反斜杠来换行with open('') as file_1, \ open('') as file_2: file_2.write(file_1.read()) 使用正确的换行位置。推荐的位置在二元操作符（binary operator，如下述代码中的and、or以及%）之后，二元运算符( + -)之前 1234567891011121314# 同一括号换行，不需要反斜杠，因为是隐式换行# 在 and or % 之后回车换行class Rectangle(Shape): def __init__(self, width, height, color='black', emphasis=None, highlight=0): if (width == 0 and height == 0 and color == 'red' and emphasis == 'strong' or height &gt; 100): raise ValueError("sorry, you lose") if width == 0 and height == 0 and (color == 'red' or emphasis is None): raise ValueError("I don't think so -- values are %s, %s" % (width, height)) Shape.__init__(self, width, height, color, emphasis, highlight) 123456# 在 + - 之前换行，增加可读性income = (gross_wages + taxable_interest + (dividends - qualified_dividends) - ira_deduction - student_loan_interest) 空行 顶级函数（当前文件中的第一个函数）或者顶级类（当前文件的第一个类）之前要有两个空行 定义在类内部的函数（成员函数）之间要留有一个空行 可以使用额外的空行（但要注意节制）以区分不同的函数组， 在一堆只有一行的函数之间不要使用空行（比如一些函数的空实现） 在函数内部使用空行，来标识不同的逻辑单元 导入import应该总是在文件的最上面，在模块注释和文档字符串之后，在模块变量和常量之前 注意import的顺序，各个import的组需要用空行隔开，顺序为（各个import独立成行） 标准库import 相关的第三方import 本地应用和库的import 一般情况使用绝对的import，但是在包层次比较复杂的时候使用相对import可以更加简洁，则使用相对import 在import一个class的时候，如果不会引起命名冲突，则可以使用from进行import，否则则直接import并且使用全名 应该避免使用利用通配符进行import，也就是避免使用from xxx import * 模块级的特殊名字，此处特指那些前后都有双下划线的名字， 比如author等，应该被放置在模块文档字符串之后，除from future import的任何import之前，比如 12345678910111213141516171819"""This is the example module.This module does stuff."""from __future__ import barry_as_FLUFL__all__ = ['a', 'b', 'c']__version__ = '0.1'__author__ = 'Cardinal Biggles'import osimport sysimport numpyimport pandasimport matplotlibimport tool 空格以下几种情况不要额外加空格： 在各种括号之中，比如spam(ham[1], {eegs: 2})而不是spam( ham[ 1 ], { eggs: 2 } ) 在逗号分号和冒号之前 但是如果冒号作为分隔符，则前后都加空格 后面立即跟了一个括号，比如函数调用的函数和括号之间不应该加空格 后面跟的是索引或者切片的中括号，比如a[1]而不是a [1] 对于赋值或者其他操作符，不要为了多个语句对齐而加很多空格，前后一个即可 其他的建议: 一行的尾部不要有空格 二元运算符前后始终都最好有一个空格 在一个表达式中有不同优先级的运算符，可以添加空格以区别优先级 在调用函数时作为参数的那个等号则前后不要有空格（虽然看起来像个二元运算符）,比如func(a=3, b=4)而不是func(a = 3, b = 4) 带箭头的函数，箭头两端也应该和二元运算符一样，前后有空格def func() -&gt; AnyStr: … 用于指示关键字参数或默认参数值时，不要在 = 符号周围使用空格 12def complex(real, imag=0.0): return magic(r=real, i=imag) 将参数注释与默认值组合时，请在=符号周围使用空格（但仅限于那些同时具有注释和默认值的参数 12def munge(sep: AnyStr = None): ...def munge(input: AnyStr, sep: AnyStr = None, limit=1000): ... 文档 为所有公共模块或者函数、类以及方法编写文档。不必为非公共方法编写doc文档，但应有一个注释描述算法的功能，这条注释应当出现在def之后 结尾的”””应当独占一行 1234&quot;&quot;&quot;Return a foobangOptional plotz says to frobnicate the bizbaz first.&quot;&quot;&quot; 注释 注释和内容冲突的，比不要注释还好，改代码一定要改注释，保证注释最新 注释应该是完整的句子，如果一个注释是一个短语或者句子，首字母大写，除非是小写字母开头的标识符作为开头 短注释结尾的句号可以省略，包含一个或多个段落的块注释每一个句子都应该有句号 在句子结束的句号之后应该有两个句号 用英语写的时候，遵守Strunk and White写作风格 除非你十分确定你的代码不会被任何不用你这个语言的人使用，否则都用英语写注释 编程推荐 代码应该不使得其他python实现有劣势，也就是不要依靠特定的python实现来提高效率 和单例(Singleton)，比如None比较，应该一直使用is和is not而不是等于符号和不等于 使用is not运算符而不是使用not … is 如果实现比较运算密集的有序操作时，最好实现所有的六种操作(eq, ne, lt, le, gt, ge) 定义有名字的函数始终使用def而不是通过lambda表达式来赋值 从Exception继承而不是从BaseException 恰当的使用exception链 在py2里边raise一个异常的时候，使用raise ValueError(‘message’)而不是raise ValuError, ‘message’， py3中后面的方法会出错 捕捉异常的时候，使用具体的异常，而不是用一个单纯的except:xxx，否则连SystemExit和KeyboardInterrupt也会被捕捉到 给异常绑定名字的时候使用py2.6添加的as的方法，except Exception as exc:… 捕捉系统错误的时候，使用py3.3添加的异常阶层而不是使用errno值 try语句包含的东西越少越好 如果一个资源是一段代码本地使用的，使用with语句保证被正确释放 上下文管理器始终应该使用函数或者方法来调用 保证返回语句一致，要么所有情况都有返回，要么都没有 使用字符串方法，而不是使用字符串模块 使用’’.startswith()和’’.endswith()而不是字符串切片来检查前缀后缀 对象类型的比较应该使用isinstance()而不是直接比较 对于序列(字符串，列表，元组)，可以利用空序列为false 字符串字面量不要依赖有意义的尾部空白 布尔值不要用==和is来比较，直接用if is_true:…而不是if is_true == True:更不要用if is_true is True:… 参考连接：官方PEP8文档 pep8 要求归纳]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python读取大文件]]></title>
    <url>%2F1232847187%2F</url>
    <content type="text"><![CDATA[python 读取大文件python读取文件一般情况是利用open()函数以及read()函数来完成： 12f = open(filename,&apos;r&apos;)f.read() 这种方法读取小文件，即读取大小远远小于内存的文件显然没有什么问题。但是如果是将一个10G大小的日志文件读取，即文件大小大于内存，这么处理就有问题了，会造成MemoryError … 也就是发生内存溢出。 这里发现跟read()类似的还有其他的方法：read(参数)、readline()、readlines() read(参数)：通过参数指定每次读取的大小长度,这样就避免了因为文件太大读取出问题。 1234while True: block = f.read(1024) if not block: break readline()：每次读取一行 1234while True: line = f.readline() if not line: break readlines()：读取全部的行，构成一个list，通过list来对文件进行处理，但是这种方式依然会造成MemoyError 12for line in f.readlines(): .... 分块读取：处理大文件是很容易想到的就是将大文件分割成若干小文件处理，处理完每个小文件后释放该部分内存。这里用了iter 和 yield： 12345678910111213141516def read_in_chunks(filePath, chunk_size=1024*1024): &quot;&quot;&quot; Lazy function (generator) to read a file piece by piece. Default chunk size: 1M You can set your own chunk size &quot;&quot;&quot; file_object = open(filePath) while True: chunk_data = file_object.read(chunk_size) if not chunk_data: break yield chunk_dataif __name__ == &quot;__main__&quot;: filePath = &apos;./path/filename&apos; for chunk in read_in_chunks(filePath): process(chunk) # &lt;do something with chunk&gt; 使用With open()：with语句打开和关闭文件，包括抛出一个内部块异常。for line in f文件对象f视为一个迭代器，会自动的采用缓冲IO和内存管理，所以你不必担心大文件 1234# If the file is line basedwith open(...) as f: for line in f: process(line) # &lt;do something with line&gt; 关于with open()的优化：面对百万行的大型数据使用with open 是没有问题的，但是这里面参数的不同也会导致不同的效率。经过测试发现参数为”rb”时的效率是”r”的6倍。由此可知二进制读取依然是最快的模式。 123with open(filename,&quot;rb&quot;) as f: for fLine in f: pass 测试结果：rb方式最快，100w行全遍历2.9秒。基本能满足中大型文件处理效率需求。如果从rb(二级制读取)读取改为r(读取模式)，慢5-6倍。 linecache模块以上几种方式都不支持对于文件按行随机访问。在这样的背景下，能够支持直接访问某一行内容的linecache模块是一种很好的补充。我们可以使用linecache模块的getline方法访问某一具体行的内容，官方文档中给出了如下用法： 12import linecachelinecache.getline(&apos;filename&apos;, 6) 在使用过程中我注意到，基于linecache的getline方法的日志分析会在跑满CPU资源之前首先占用大量内存空间，也就是在CPU使用率仍然很低的情况下，内存空间就会被迅速地消耗。这一现象引起了我的兴趣。我猜测linecache在随机读取文件时，是首先依序将文件读入内存，之后寻找所要定位的行是否在内存当中。若不在，则进行相应的替换行为，直至寻找到所对应的行，再将其返回。对linecache代码的阅读证实了这一想法。在linecache.py中，我们可以看到getline的定义为： 123456def getline(filename, lineno, module_globals=None): lines = getlines(filename, module_globals) if 1 &lt;= lineno &lt;= len(lines): return lines[lineno-1] else: return &apos;&apos; 不难看出，getline方法通过getlines得到了文件行的List，以此来实现对于文件行的随机读取。继续查看getlines的定义。 12345678def getlines(filename, module_globals=None): &quot;&quot;&quot;Get the lines for a file from the cache. Update the cache if it doesn&apos;t contain an entry for this file already.&quot;&quot;&quot; if filename in cache: return cache[filename][2] else: return updatecache(filename, module_globals) 由此可见，getlines方法会首先确认文件是否在缓存当中，如果在则返回该文件的行的List，否则执行updatecache方法，对缓存内容进行更新。因此，在程序启动阶段，linecache不得不首先占用内存对文件进行缓存，才能进行后续的读取操作。而在updatecache方法中，我们可以看到一个有趣的事实是： 12345678910111213141516171819def updatecache(filename, module_globals=None): &quot;&quot;&quot;Update a cache entry and return its list of lines. If something&apos;s wrong, print a message, discard the cache entry, and return an empty list.&quot;&quot;&quot; ## ... 省略... try: fp = open(fullname, &apos;rU&apos;) lines = fp.readlines() fp.close() except IOError, msg:## print &apos;*** Cannot open&apos;, fullname, &apos;:&apos;, msg return [] if lines and not lines[-1].endswith(&apos;\n&apos;): lines[-1] += &apos;\n&apos; size, mtime = stat.st_size, stat.st_mtime cache[filename] = size, mtime, lines, fullname return lines 也就是说，linecache依然借助了文件对象的readlines方法。这也给了我们一个提示，当文件很大不适用readlines方法直接获取行的List进行读取解析时，linecache似乎也并不会成为一个很好的选择。 内存检测工具介绍memory_profiler首先先用pip安装memory_profiler 1pip install memory_profiler memory_profiler是利用python的装饰器工作的，所以我们需要在进行测试的函数上添加装饰器。 123456789101112131415from hashlib import sha1import sys@profiledef my_func(): sha1Obj = sha1() with open(sys.argv[1], &apos;rb&apos;) as f: while True: buf = f.read(1024 * 1024) if buf: sha1Obj.update(buf) else: break print(sha1Obj.hexdigest())if __name__ == &apos;__main__&apos;: my_func() 之后在运行代码时加上 -m memory_profiler 就可以了解函数每一步代码的内存占用了 guppy首先，通过pip先安装guppy 1pip install guppy 之后可以在代码之中利用guppy直接打印出对应各种python类型（list、tuple、dict等）分别创建了多少对象，占用了多少内存。 1234567891011from guppy import hpyimport sysdef my_func(): mem = hpy() with open(sys.argv[1], &apos;rb&apos;) as f: while True: buf = f.read(10 * 1024 * 1024) if buf: print(mem.heap()) else: break 通过上述两种工具guppy与memory_profiler可以很好地来监控python代码运行时的内存占用问题。 参考链接：Python读取大文件的”坑“与内存占用检测 使用Python读取大文件的方法 python linecache读取过程]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[celery]]></title>
    <url>%2F1699526646%2F</url>
    <content type="text"><![CDATA[celery 和 Flask + redisCelery 是一个简单、灵活且可靠的，处理大量消息的分布式系统，并且提供维护这样一个系统的必需工具。 它是一个专注于实时处理的任务队列，同时也支持任务调度。 Celery 需要一个发送和接受消息的传输者。RabbitMQ 和 Redis 中间人的消息传输支持所有特性，但也提供大量其他实验性方案的支持，包括用 SQLite 进行本地开发。 Celery 可以单机运行，也可以在多台机器上运行，甚至可以跨越数据中心运行。 celery 配置文件目录树： 123456789101112131415server/├── apis│ └── api.raml├── xxxx│ ├── xxxx.py│ ├── models.py│ ├── transforms.py│ ├── users.py│ └── views│ ├── __init__.py│ ├── tasks.py│ └── xxxx.py├── celery_worker.py├── run.py└── settings.py 在settings.py文件中配置cellery 1234class Config(object): # celery config CELERY_BROKER_URL = &apos;redis://:@localhost:6379/&apos; CELERY_RESULT_BACKEND = &apos;redis://:@localhost:6379/&apos; 在init.py文件下和app一起配置： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879from flask import Flask, render_template, redirect, url_for,\ _request_ctx_stack, request, abortfrom flask.ext.security import Security, SQLAlchemyUserDatastorefrom .models import db, User, Rolefrom celery import Celeryimport loggingCELERY = Celery(&apos;ddu&apos;,broker=&apos;redis://:@localhost:6379/1&apos;)def _log_config(app): if not app.debug and not app.testing: from logging.handlers import RotatingFileHandler file_handler = RotatingFileHandler( app.config.get(&apos;LOGGING_PATH&apos;), maxBytes=app.config.get(&apos;LOGGING_SIZE&apos;)) file_handler.setLevel(logging.WARNING) app.logger.addHandler(file_handler)def create_app(config): app = Flask( __name__, template_folder=&apos;../../votingmanage/templates&apos;, static_folder=&apos;../../votingmanage/dist&apos;, static_url_path=&quot;/static&quot;) app.config.from_object(config) app.db = db db.init_app(app) user_datastore = SQLAlchemyUserDatastore(db, User, Role) security = Security(app, user_datastore) app.user_datastore = user_datastore @app.route(&apos;/&apos;) def to_app(): return redirect(url_for(&apos;ngapp.home&apos;)) app.apns = APNs(use_sandbox=True, cert_file=path.join(app.root_path, &apos;ck.pem&apos;), key_file=path.join(app.root_path, &apos;ck.pem&apos;)) import users app.register_blueprint(users.bp, url_prefix=&apos;/api/users&apos;) # Poplulate user information if the token is specified @app.before_request def populate_user(): header_key = app.config.get(&apos;SECURITY_TOKEN_AUTHENTICATION_HEADER&apos;, &apos;Authentication-Token&apos;) args_key = app.config.get(&apos;SECURITY_TOKEN_AUTHENTICATION_KEY&apos;, &apos;token&apos;) header_token = request.headers.get(header_key, None) token = request.args.get(args_key, header_token) if request.get_json(silent=True): token = request.json.get(args_key, token) if token: user = app.extensions[&apos;security&apos;].login_manager.token_callback(token) _request_ctx_stack.top.user = user global CELERY #全局变量CELERY def make_celery(app): #此处app为创建的实例对象 celery = Celery(app.import_name, broker=app.config[&apos;CELERY_BROKER_URL&apos;], backend=app.config[&apos;CELERY_RESULT_BACKEND&apos;]) celery.conf.update(app.config) TaskBase = celery.Task class ContextTask(TaskBase): abstract = True def __call__(self, *args, **kwargs): with app.app_context(): return TaskBase.__call__(self, *args, **kwargs) celery.Task = ContextTask return celery CELERY = make_celery(app) return app 创建celery的启动文件celery_worker.py： 12345from ddu import create_app, CELERYfrom settings import DEV as PRODapp = create_app(PROD)app.app_context().push() 编写tasks.py文件，完成定时任务： 12345678910111213141516171819202122232425# -*- coding:utf-8 -*-from .. import CELERYfrom ..models import User, Work, Remid, GZHUserfrom datetime import datetime,timedeltafrom flask import current_app, requestimport requestsimport json@CELERY.on_after_configure.connectdef setup_periodic_tasks(sender, **kwargs): # 添加定时任务，设置时间 # sender.add_periodic_task(10.0, test1.s(&apos;world&apos;), expires=10) sender.add_periodic_task(60.0, weixin_remid.s()) sender.add_periodic_task(10.0, user_unionid.s()) sender.add_periodic_task(120.0, get_list.s()) @CELERY.taskdef test1(arg1=&quot;arg1&quot;): print arg1,111 return 5 @CELERY.taskdef user_unionid(): count = get_people_unionid() print str(count) + &apos;user unionid...&apos; 编写xxx.py文件，完成异步任务： 1234task.add.apply_async(args=[3,7],expires=10)# expires：任务过期时间，参数类型可以是 int，也可以是 datetime# eta (estimated time of arrival)：指定任务被调度的具体时间，参数类型是 datetime# countdown：指定多少秒后执行任务 celery 启动在celery_worker.py文件下： celery -A celery_worker.CELERY worker –loglevel=info –beat 注意： 编写完tasks.py文件后，在启动Flask项目时，应该在主文件中导入tasks.py文件 from .tasks import test1 定时任务，也可以在配置文件中进行配置描述（此方法未试验成功，setting配置无响应） 1234567891011from datetime import timedeltaCELERYBEAT_SCHEDULE = &#123; &apos;add-every-30-seconds&apos;: &#123; &apos;task&apos;: &apos;tasks.add&apos;, &apos;schedule&apos;: timedelta(seconds=30), &apos;args&apos;: (16, 16) &#125;,&#125;CELERY_TIMEZONE = &apos;UTC&apos; 12345678910from celery.schedules import crontabCELERYBEAT_SCHEDULE = &#123; # Executes every Monday morning at 7:30 A.M &apos;add-every-monday-morning&apos;: &#123; &apos;task&apos;: &apos;tasks.add&apos;, &apos;schedule&apos;: crontab(hour=7, minute=30, day_of_week=1), &apos;args&apos;: (16, 16), &#125;,&#125; celery 和 Flask 同时使用时一定要注意实例的config配置问题，以免出现混乱 上文为celery读取数据库进行定时任务（未涉及任务结果的重新保存）]]></content>
      <categories>
        <category>组件使用</category>
      </categories>
      <tags>
        <tag>组件</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[logging 日志模块]]></title>
    <url>%2F2728557103%2F</url>
    <content type="text"><![CDATA[logging 日志系统[toc] 日志级别 等级 数值 描述 NOTSET 0 DEBUG 10 详细信息，通常仅在诊断问题时才有意义 INFO 20 确认事情按预期工作 WARNING 30 警告信息 ERROR 40 由于更严重的问题，该软件无法执行某些功能 CRITICAL 50 严重错误，表明程序本身可能无法继续运行 默认级别为 WARNING，这意味着将仅跟踪此级别及更高级别的事件，除非日志包已配置为执行其他操作。 记录器Logger对象有三重的工作。首先，它们向应用程序代码公开了几种方法，以便应用程序可以在运行时记录消息 其次，记录器对象根据严重性（默认过滤工具）或过滤器对象确定要处理的日志消息。第三，记录器对象将相关的日志消息传递给所有感兴趣的日志处理程序。 记录器对象上使用最广泛的方法分为两类：配置和消息发送。 Logger.setLevel() 指定记录器将处理的最低严重性日志消息，其中debug是最低内置严重性级别，critical是最高内置严重性级别。例如，如果严重性级别为INFO，则记录器将仅处理INFO，WARNING，ERROR和CRITICAL消息，并将忽略DEBUG消息。 Logger.addHandler()并Logger.removeHandler()从logger对象添加和删除处理程序对象。处理程序中详细介绍了处理程序。 Logger.addFilter()并Logger.removeFilter()从记录器对象中添加和删除过滤器对象。过滤器对象中详细介绍了 过滤器。 处理程序Handler对象负责将适当的日志消息（基于日志消息的严重性）分派给处理程序的指定目标。 Logger对象可以使用addHandler()方法向自己添加零个或多个处理程序对象。作为示例场景，应用程序可能希望将所有日志消息发送到日志文件，将错误或更高的所有日志消息发送到标准输出，以及对电子邮件地址至关重要的所有消息。此方案需要三个单独的处理程序，其中每个处理程序负责将特定严重性的消息发送到特定位置。 标准库包含很多处理程序类型（请参阅 有用的处理程序）; 教程主要使用StreamHandler并 FileHandler在其示例中使用。 处理程序中很少有方法可供应用程序开发人员关注。与使用内置处理程序对象（即不创建自定义处理程序）的应用程序开发人员相关的唯一处理程序方法是以下配置方法： 该setLevel()方法与logger对象一样，指定将分派到适当目标的最低严重性。为什么有两种setLevel()方法？记录器中设置的级别确定将传递给其处理程序的消息的严重性。每个处理程序中设置的级别确定处理程序将发送哪些消息。 setFormatter() 选择要使用的此处理程序的Formatter对象。 addFilter()并removeFilter()分别在处理程序上配置和取消配置过滤器对象。 应用程序代码不应直接实例化和使用实例 Handler。相反，Handler该类是一个基类，它定义了所有处理程序应具有的接口，并建立了子类可以使用（或覆盖）的一些默认行为 常见处理程序除了基Handler类之外，还提供了许多有用的子类： StreamHandler 实例将消息发送到流（类文件对象）。 FileHandler 实例将消息发送到磁盘文件。 BaseRotatingHandler是在某个点旋转日志文件的处理程序的基类。它并不意味着直接实例化。相反，使用RotatingFileHandler或 TimedRotatingFileHandler。 RotatingFileHandler 实例将消息发送到磁盘文件，支持最大日志文件大小和日志文件轮换。 TimedRotatingFileHandler 实例将消息发送到磁盘文件，以特定的时间间隔旋转日志文件。 SocketHandler实例将消息发送到TCP / IP套接字。从3.4开始，也支持Unix域套接字。 DatagramHandler实例将消息发送到UDP套接字。从3.4开始，也支持Unix域套接字。 SMTPHandler 实例将消息发送到指定的电子邮件地址。 SysLogHandler 实例将消息发送到Unix syslog守护程序，可能在远程计算机上。 NTEventLogHandler 实例将消息发送到Windows NT / 2000 / XP事件日志。 MemoryHandler 实例将消息发送到内存中的缓冲区，只要满足特定条件，就会刷新。 HTTPHandler实例使用任一语义GET或POST语义将消息发送到HTTP服务器。 WatchedFileHandler实例会监视他们要登录的文件。如果文件发生更改，则会关闭该文件并使用文件名重新打开。此处理程序仅在类Unix系统上有用; Windows不支持使用的基础机制。 QueueHandler实例将消息发送到队列，例如在queue或multiprocessing模块中实现的那些队列。 NullHandler实例不执行任何错误消息。它们由想要使用日志记录的库开发人员使用，但是希望避免“如果库用户没有配置日志记录，则可以显示”没有找到记录器XXX的处理程序“消息。有关更多信息，请参阅配置库的日志记录。 新的3.1版：的NullHandler类。 新版本3.2：在QueueHandler类。 基本配置 logging.basicConfig(filename=’example.log’, filemode=’w’, level=logging.DEBUG) 基本配置过程为：获取logger，获取对应Handler并设置参数，然后将Handler添加到logger，有些模块包含创建logger的方法 在flask中的配置 1234567891011121314151617import loggingdef _log_config(app): if not app.debug and not app.testing: from logging.handlers import RotatingFileHandler file_handler = RotatingFileHandler( app.config.get(&apos;LOGGING_PATH&apos;), maxBytes=app.config.get(&apos;LOGGING_SIZE&apos;)) file_handler.setLevel(logging.WARNING) app.logger.addHandler(file_handler) # 然后在创建app的时候执行此函数 _log_config(app)def create_app(config): app = Flask() ... _log_config(app) ... 参考连接： Logging Cookbook Logging handlers logging flask logging]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL数据库]]></title>
    <url>%2F2653527150%2F</url>
    <content type="text"><![CDATA[MySQL数据库相关操作1.安装MySQL数据库查看mysql支持的存储引擎，”show engines;” Engine Support Comment Transactions XA Savepoints MyISAM YES MyISAM storage engine NO NO NO CSV YES CSV storage engine NO NO NO MRG_MYISAM YES Collection of identical MyISAM tables NO NO NO BLACKHOLE YES /dev/null storage engine (anything you write to it disappears) NO NO NO PERFORMANCE_SCHEMA YES Performance Schema NO NO NO InnoDB DEFAULT Supports transactions, row-level locking, and foreign keys YES YES YES ARCHIVE YES Archive storage engine NO NO NO MEMORY YES Hash based, stored in memory, useful for temporary tables NO NO NO FEDERATED NO Federated MySQL storage engine NULL NULL NULL 关于存储引擎的介绍：四种mysql存储引擎 DEFAULT为默认存储引擎，因为MyISAM引擎（一般为默认）不支持事务也不支持外键，所以需要用到Innodb引擎，于是将mysql的默认引擎设置为innodb 在配置文件my.cnf中的 [mysqld] 下面修改default-storage-engine=INNODB，如果没有则添加 重启mysql服务器：service mysqld restart 查看数据库默认使用的存储引擎： show variables like ‘%storage_engine%’; 查看已有表的存储引擎： show create table Tablename(表名); 2.设置MySQL数据库的编码格式utf-8编码可能2个字节、3个字节、4个字节的字符，但是MySQL的utf8编码只支持3字节的数据，而移动端的表情数据是4个字节的字符。如果直接往采用utf-8编码的数据库中插入表情数据，Java程序中将报SQL异常 可以对4字节的字符进行编码存储，然后取出来的时候，再进行解码。但是这样做会使得任何使用该字符的地方都要进行编码与解码。 utf8mb4编码是utf8编码的超集，兼容utf8，并且能存储4字节的表情字符。采用utf8mb4编码的好处是：存储与获取数据的时候，不用再考虑表情字符的编码与解码问题 更改数据库的编码为utf8mb4MySQL的版本:utf8mb4的最低mysql版本支持版本为5.5.3+，若不是，请升级到较新版本 修改MySQL配置文件: 修改mysql配置文件my.cnf（windows为my.ini） my.cnf一般在etc/mysql/my.cnf位置。找到后请在以下三部分里添加如下内容： [client] default-character-set = utf8mb4 [mysql] default-character-set = utf8mb4 [mysqld] character-set-client-handshake = FALSE character-set-server = utf8mb4 collation-server = utf8mb4_unicode_ci init_connect=’SET NAMES utf8mb4’ 重启数据库，检查变量 SHOW VARIABLES WHERE Variable_name LIKE ‘character_set_%’ OR Variable_name LIKE ‘collation%’; Variable_name Value 描述 character_set_client utf8mb4 客户端来源数据使用的字符集 character_set_connection utf8mb4 连接层字符集 character_set_database utf8mb4 当前选中数据库的默认字符集 character_set_filesystem binary character_set_results utf8mb4 查询结果字符集 character_set_server utf8mb4 默认的内部操作字符集 character_set_system utf8 collation_connection utf8mb4_unicode_ci collation_database utf8mb4_unicode_ci collation_server utf8mb4_unicode_ci 数据库连接的配置 数据库连接参数中: characterEncoding=utf8会被自动识别为utf8mb4，也可以不加这个参数，会自动检测。 而autoReconnect=true是必须加上的。 将数据库和已经建好的表也转换成utf8mb4 更改数据库编码：ALTER DATABASE caitu99 CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci; 更改表编码：ALTER TABLE TABLE_NAME CONVERT TO CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci; 如有必要，还可以更改列的编码 3.更改数据库的存储路径在mysql下查看数据存储路径(默认位置如下)12345678mysql&gt; show variables like &apos;%datadir%&apos;;+---------------+-----------------+| Variable_name | Value |+---------------+-----------------+| datadir | /var/lib/mysql/ |+---------------+-----------------+1 row in set (0.04 sec) 关闭mysql [root@mysql ~]# systemctl stop mysql //停止mysql [root@mysql ~]# mv /var/lib/mysql /var/lib/mysql.BAK //数据改名备份一下，方便回滚 [root@mysql ~]# mkdir /data //这里新建一个假设的新目录/data/ [root@mysql ~]# rsync -av /var/lib/mysql /data/ //复制数据到新目录。rsync命令没有可以用yum安装；也可以用cp命令复制;此处也有很多人在用mv直接移动,省去权限问题的麻烦,如果直接用mv记得要备份好原数据 更改my.cnf文件 [root@mysql ~]# vim /etc/my.cnf //编辑 my.cnf。如果默认没有，可以”cp /usr/share/mysql/my-default.cnf /etc/my.cnf” [client] port = 3306socket = /data/mysql/mysql.sock [mysqld] datadir = /data/mysql socket = /data/mysql/mysql.sock 备注：其实socke可以不用改的 只要改下 datadir就行了 如果安装innodb 那么 innodbdir 也要改 [mysqld_safe] socket = /home/mysql/mysql.sock [mysql.server] socket = /home/mysql/mysql.sock 再次启动mysql [root@mysql ~]# systemctl start mysql 启动失败解决办法 在出现报错情况下，优先去查看日志去分析问题 检查selinux 一个简单的解决办法是使用命令暂时关闭selinux，以便让你的操作可以继续下去 setenforce 0 但最好使用一个永久方法，以便在重启后继续不要这货 修改/etc/selinux/config文件中设置SELINUX=disabled ，然后重启或等待下次重启。（如果没有这个就看下面的方法) 检查apparmor 它也对mysql所能使用的目录权限做了限制 在 /etc/apparmor.d/usr.sbin.mysqld 这个文件中，有这两行，规定了mysql使用的数据文件路径权限 /var/lib/mysql/ r, /var/lib/mysql/** rwk, 我想把数据文件移动到/data/mysql下，那么为了使mysqld可以使用/data/mysql这个目录，照上面那两条，增加下面这两条就可以了 /data/mysql/ r, /data/mysql/** rwk, 重启: sudo service apparmor restart 如有必要,将socket新目录也添加上]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python 虚拟环境的搭建]]></title>
    <url>%2F945033781%2F</url>
    <content type="text"><![CDATA[python 虚拟环境的创建基于python2 的虚拟环境搭建virtualenv 是一个创建隔绝的Python环境的工具，用来管理虚拟环境 安装 virtualenv： pip install virtualenv 创建虚拟环境： 12345678910$ cd my_project_dir$ virtualenv venv #venv为虚拟环境目录名，目录名自定义&apos;&apos;&apos;激活虚拟环境：&apos;&apos;&apos;$ source venv/bin/activate&apos;&apos;&apos;安装需要的包：&apos;&apos;&apos;$ pip install requests&apos;&apos;&apos;当有requirements.txt的时候可以&apos;&apos;&apos;$ pip install -r requirements.txt #直接安装全部包&apos;&apos;&apos;退出虚拟环境：&apos;&apos;&apos;$ . venv/bin/deactivate 基于python3 的虚拟环境搭建使用venv搭建的虚拟环境同virtualenv搭建的虚拟环境，即venv可替代virtualenv python3自带venv 创建虚拟环境：123456789101112$ mkdir myenv #venv为虚拟环境目录名，目录名自定义$ cd myenv$ python3 -m venv . #(venv之后一个空格加上一个点 “ . ”)&apos;&apos;&apos;激活虚拟环境：&apos;&apos;&apos;$ cd Scripts$ activate.bat / activate&apos;&apos;&apos;安装需要的包：&apos;&apos;&apos;$ pip3 install requests #requests 即为包名，需要版本号时为：request==版本号&apos;&apos;&apos;退出虚拟环境：&apos;&apos;&apos;$ deactivate.bat / deactivate pip3升级：1234sudo pip3 install --upgrade pipsudo vim /usr/bin/pip3 from pip import __main__ ......(__main__._main())]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker 容器搭建]]></title>
    <url>%2F3677998467%2F</url>
    <content type="text"><![CDATA[docker 学习笔记docker 安装 在ubuntu下的安装 sudo apt install docker.io 在生产环境下应该安装指定版本 sudo apt-get install docker-ce= 注意：在此处可能会出现包引用的错误，具体原因自行解决（没搞懂…） 在win下安装 在windows下安装需要满足的条件： Docker for Windows 支持 64 位版本的 Windows 10 Pro，且必须开启 Hyper-V。 Windows 10 Pro, Windows旗舰版和教育版可行。 Hyper-V 为Windows上的虚拟机，如果电脑上还有VM虚拟机，则这两个会发生冲突，VM在运行时需要关闭Hyper-V。 在mac下安装 参考win下安装 docker 需要用户具有sudo权限，为了避免每次都输入sudo，可以添加用户到docker用户组 创建用户组,添加用户，重启： $ sudo groupadd docker $ sudo gpasswd -a ${USER} docker $ sudo service docker restart 切换当前会话到新 group 或者重启 X 会话 newgrp - docker 如果已有分组，直接添加用户 $ sudo usermod -aG docker $USER docker 镜像（Image）镜像加速： Docker 官方提供的中国 registry mirror https://registry.docker-cn.com 七牛云加速器 https://reg-mirror.qiniu.com/ 对于使用 upstart 的系统而言，编辑 /etc/default/docker 文件，在其中的 DOCKER_OPTS 中配置加速器地址： DOCKER_OPTS=”–registry-mirror=https://registry.docker-cn.com&quot; 重新启动服务。 $ sudo service docker restart 对于使用 systemd 的系统，请在 /etc/docker/daemon.json 中写入如下内容（如果文件不存在请新建该文件） 12345&#123; &quot;registry-mirrors&quot;: [ &quot;https://registry.docker-cn.com&quot; ]&#125; 之后重新启动服务。 $ sudo systemctl daemon-reload$ sudo systemctl restart docker 对于使用 Windows 10 的系统，在系统右下角托盘 Docker 图标内右键菜单选择 Settings，打开配置窗口后左侧导航菜单选择 Daemon。在 Registry mirrors 一栏中填写加速器地址 https://registry.docker-cn.com，之后点击 Apply 保存后 Docker 就会重启并应用配置的镜像地址了 获取镜像： $ docker pull [选项] [Docker Registry 地址[:端口号]/]仓库名[:标签] 一般从Docker Store上下载，搜索需要的镜像并按Tag下载，一般为官方原版镜像 也可以自己创建自定义镜像，创建Dockerfile 12345678910111213141516171819202122FROM python:3.6-alpine3.8 # 官方基于alpine3.8下的python3.6 镜像# alpine为精简版Linux# 在此基础上叠加镜像包LABEL maintainer=&quot;Seven test&quot;# 创建人信息COPY ./app /usr/src/app# 拷贝本地文件./app 到镜像文件/use/src/app下RUN cd /usr/src/app &amp;&amp; pip install -r requirements.txt# 执行操作 进入/usr/src/app 并下载相关安装包WORKDIR /usr/src/app# 指定工作目录EXPOSE 8888# 对外暴露的端口CMD python xxx/xxx.py# 启动容器时执行的操作 列出镜像： $ docker image ls 删除镜像： $ docker image rm [选项] &lt;镜像1&gt; [&lt;镜像2&gt; …] docker 容器（Container）每一个容器运行时，是以镜像为基础层，在其上创建一个当前容器的存储层，我们可以称这个为容器运行时读写而准备的存储层为容器存储层。容器存储层的生存周期和容器一样，容器消亡时，容器存储层也随之消亡。因此，任何保存于容器存储层的信息都会随容器删除而丢失。 按照 Docker 最佳实践的要求，容器不应该向其存储层内写入任何数据，容器存储层要保持无状态化。所有的文件写入操作，都应该使用 数据卷（Volume）、或者绑定宿主目录，在这些位置的读写会跳过容器存储层，直接对宿主（或网络存储）发生读写，其性能和稳定性更高。 数据卷的生存周期独立于容器，容器消亡，数据卷不会消亡。因此，使用数据卷后，容器删除或者重新运行之后，数据却不会丢失。 运行容器： $ sudo docker run –name webserver -d -p 80:80 nginx $ docker exec -it webserver bash 当然，你应该还会用到其它的一些参数，比如-name指定容器名字，后面再 start 这个容器就不用查 id 了；-v挂载文件，将你本地的代码挂载进 docker；或者-p映射端口，将 docker 的端口映射到本机，以便提供http 等服务。 例如，创建一个名字为 webserver的镜像，将我本地 code下的代码挂载到镜像/root/app目录下，并将虚拟机的80端口映射到本机8080，命令如下： docker run -it –name webserver -v ~/code:/root/app -p 8080:80 ubuntu:16.04 /bin/bash Docker Compose 的使用在多容器连接的情况下，避免重复去创建容器，所以采用docker-compose更方便 Linux下需要另外下载docker-compose: $ sudu apt install docker-compose 在项目文件下新建 docker-compose.yml 文件： docker-compose 遵循YAML格式 12345678910111213141516mysql: image: mysql:5.7 environment: - MYSQL_ROOT_PASSWORD=123456 - MYSQL_DATABASE=wordpressweb: image: wordpress links: - mysql environment: - WORDPRESS_DB_PASSWORD=123456 ports: - &quot;127.0.0.3:8080:80&quot; working_dir: /var/www/html volumes: - wordpress:/var/www/html 两个顶层标签表示有两个容器mysql和web。每个容器的具体设置，前面都已经讲解过了，还是挺容易理解的。 启动所有服务 $ docker-compose up 关闭所有服务 $ docker-compose stop 关闭以后，这两个容器文件还是存在的，写在里面的数据不会丢失。下次启动的时候，还可以复用。下面的命令可以把这两个容器文件删除（容器必须已经停止运行）。 $ docker-compose rm 注意点： 参考文档：Docker —— 从入门到实践 docker docs]]></content>
      <categories>
        <category>环境搭建</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[screen操作笔记]]></title>
    <url>%2F2547638477%2F</url>
    <content type="text"><![CDATA[screen 操作笔记简介：Screen是一款由GNU计划开发的用于命令行终端切换的自由软件。用户可以通过该软件同时连接多个本地或远程的命令行会话，并在其间自由切换。GNU Screen可以看作是窗口管理器的命令行界面版本。它提供了统一的管理多个会话的界面和相应的功能。 安装screen有些Linux会自带screen，如果没有可以在GNU screen官网下载 [seven@TT ~]# yuminstall screen [seven@TT ~]# sudo apt install screen 也可以从官网下载安装包，解压安装 GNU screen官网 操作命令：创建一个新窗口安装完成后，直接敲命令screen就可以启动它。但是这样启动的screen会话没有名字，实践上推荐为每个screen会话取一个名字，方便分辨： seven@TT:~/Desktop$ screen -S xxx 也可以直接加上所要运行的脚本程序 seven@TT:~/Desktop$ screen vim aaa.txt seven@TT:~/Desktop$ screen -S acac vim bbb.txt screen创建一个执行vim aaa.txt的单窗口会话，退出vim 将退出该窗口/会话。 需要长时间运行的后台程序 seven@TT:~/Desktop$ screen python run.py 会话分离与恢复screen 的好处就是可以在不中断程序运行的状态,而暂时断开会话窗口,在随后可以重新连接该会话,重新控制运行的程序1234567891011seven@TT:~/Desktop$ screen -S xxx[detached from 27802.xxx]&apos;&apos;&apos;创建并进入会话&apos;&apos;&apos;&apos;&apos;&apos;在会话窗口按 Ctrl+a+d 可以不中断程序,而暂时退出会话&apos;&apos;&apos;seven@TT:~/Desktop$ screen -lsThere is a screen on: 27802.xxx (2018年11月20日 10时15分57秒) (Detached)1 Socket in /var/run/screen/S-seven.&apos;&apos;&apos;查看会话列表&apos;&apos;&apos;seven@TT:~/Desktop$ screen -r xxx&apos;&apos;&apos;重新连接指定会话&apos;&apos;&apos; 当然，如果你在另一台机器上没有分离一个Screen会话，就无从恢复会话了。这时可以使用下面命令强制将这个会话从它所在的终端分离，转移到新的终端上来：12345678910111213seven@TT:~/Desktop$ screen -lsThere are screens on: 28018.awaw (2018年11月20日 10时40分51秒) (Attached) 27869.acac (2018年11月20日 10时22分53秒) (Detached) 27802.xxx (2018年11月20日 10时15分58秒) (Detached)3 Sockets in /var/run/screen/S-seven.&apos;&apos;&apos;Attached 为未分离的会话&apos;&apos;&apos;seven@TT:~/Desktop$ screen -d 28018[28018.awaw detached.]&apos;&apos;&apos;强制分离会话&apos;&apos;&apos;seven@TT:~/Desktop$ screen -r 28018[detached from 28018.awaw]&apos;&apos;&apos;重新连接会话&apos;&apos;&apos; 清除dead 会话如果由于某种原因其中一个会话死掉了（例如人为杀掉该会话），这时screen -list会显示该会话为dead状态。使用screen -wipe命令清除该会话： 1234567891011121314151617181920212223242526seven@TT:~/Desktop$ screen -lsThere are screens on: 28018.awaw (2018年11月20日 10时40分51秒) (Detached) 27869.acac (2018年11月20日 10时22分53秒) (Detached) 27802.xxx (2018年11月20日 10时15分58秒) (Detached)3 Sockets in /var/run/screen/S-seven.seven@TT:~/Desktop$ kill -9 27802seven@TT:~/Desktop$ screen -lsThere are screens on: 28018.awaw (2018年11月20日 10时40分50秒) (Detached) 27869.acac (2018年11月20日 10时22分52秒) (Detached) 27802.xxx (2018年11月20日 09时44分52秒) (Dead ???)Remove dead screens with &apos;screen -wipe&apos;.3 Sockets in /var/run/screen/S-seven.seven@TT:~/Desktop$ screen -wipe 27802There is a screen on: 27802.xxx (2018年11月20日 09时44分53秒) (Removed)1 socket wiped out.No Sockets found in /var/run/screen/S-seven.seven@TT:~/Desktop$ screen -lsThere are screens on: 28018.awaw (2018年11月20日 10时40分51秒) (Detached) 27869.acac (2018年11月20日 10时22分53秒) (Detached)2 Sockets in /var/run/screen/S-seven. 关闭或杀死窗口正常情况下，当你退出一个窗口中最后一个程序（通常是bash）后，这个窗口就关闭了。另一个关闭窗口的方法是使用C-a k，这个快捷键杀死当前的窗口，同时也将杀死这个窗口中正在运行的进程。 如果一个Screen会话中最后一个窗口被关闭了，那么整个Screen会话也就退出了，screen进程会被终止。 除了依次退出/杀死当前Screen会话中所有窗口这种方法之外，还可以使用快捷键C-a :，然后输入quit命令退出Screen会话。需要注意的是，这样退出会杀死所有窗口并退出其中运行的所有程序。其实C-a :这个快捷键允许用户直接输入的命令有很多，包括分屏可以输入split等，这也是实现Screen功能的一个途径，不过个人认为还是快捷键比较方便些 会话共享 seven@TT:~/Desktop$ screen -x 假设你在和朋友在不同地点以相同用户登录一台机器，然后你创建一个screen会话，你朋友可以在他的终端上命令,这个命令会将你朋友的终端Attach到你的Screen会话上，并且你的终端不会被Detach。这样你就可以和朋友共享同一个会话了，如果你们当前又处于同一个窗口，那就相当于坐在同一个显示器前面，你的操作会同步演示给你朋友，你朋友的操作也会同步演示给你。当然，如果你们切换到这个会话的不同窗口中去，那还是可以分别进行不同的操作的 会话锁定与解锁Screen允许使用快捷键C-a s锁定会话。锁定以后，再进行任何输入屏幕都不会再有反应了。但是要注意虽然屏幕上看不到反应，但你的输入都会被Screen中的进程接收到。快捷键C-a q可以解锁一个会话。 也可以使用C-a x锁定会话，不同的是这样锁定之后，会话会被Screen所属用户的密码保护，需要输入密码才能继续访问这个会话。 屏幕分割现在显示器那么大，将一个屏幕分割成不同区域显示不同的Screen窗口显然是个很酷的事情。可以使用快捷键C-a S将显示器水平分割，Screen 4.00.03版本以后，也支持垂直分屏，快捷键是C-a |。分屏以后，可以使用C-a 在各个区块间切换，每一区块上都可以创建窗口并在其中运行进程。 可以用C-a X快捷键关闭当前焦点所在的屏幕区块，也可以用C-a Q关闭除当前区块之外其他的所有区块。关闭的区块中的窗口并不会关闭，还可以通过窗口切换找到它 C/P模式和操作screen的另一个很强大的功能就是可以在不同窗口之间进行复制粘贴了。使用快捷键C-a 或者C-a [可以进入copy/paste模式，这个模式下可以像在vi中一样移动光标，并可以使用空格键设置标记。其实在这个模式下有很多类似vi的操作，譬如使用/进行搜索，使用y快速标记一行，使用w快速标记一个单词等。关于C/P模式下的高级操作，其文档的这一部分有比较详细的说明。 一般情况下，可以移动光标到指定位置，按下空格设置一个开头标记，然后移动光标到结尾位置，按下空格设置第二个标记，同时会将两个标记之间的部分储存在copy/paste buffer中，并退出copy/paste模式。在正常模式下，可以使用快捷键C-a ]将储存在buffer中的内容粘贴到当前窗口 更多screen功能同大多数UNIX程序一样，GNU Screen提供了丰富强大的定制功能。你可以在Screen的默认两级配置文件/etc/screenrc和$HOME/.screenrc中指定更多，例如设定screen选项，定制绑定键，设定screen会话自启动窗口，启用多用户模式，定制用户访问权限控制等等。如果你愿意的话，也可以自己指定screen配置文件。 以多用户功能为例，screen默认是以单用户模式运行的，你需要在配置文件中指定multiuser on 来打开多用户模式，通过acl*（acladd,acldel,aclchg…）命令，你可以灵活配置其他用户访问你的screen会话 命令查看 # screen [-AmRvx -ls -wipe][-d &lt;作业名称&gt;][-h &lt;行数&gt;][-r &lt;作业名称&gt;][-s ][-S &lt;作业名称&gt;] 命令行参数-A 将所有的视窗都调整为目前终端机的大小。 -d &lt;作业名称&gt; 将指定的screen作业离线。 -h &lt;行数&gt; 指定视窗的缓冲区行数。 -m 即使目前已在作业中的screen作业，仍强制建立新的screen作业。 -r &lt;作业名称&gt; 恢复离线的screen作业。 -R 先试图恢复离线的作业。若找不到离线的作业，即建立新的screen作业。 -s 指定建立新视窗时，所要执行的shell。 -S &lt;作业名称&gt; 指定screen作业的名称。 -v 显示版本信息。 -x 恢复之前离线的screen作业。 -ls或–list 显示目前所有的screen作业。 -wipe 检查目前所有的screen作业，并删除已经无法使用的screen作业 窗口命令C-a ? -&gt; 显示所有键绑定信息C-a c -&gt; 创建一个新的运行shell的窗口并切换到该窗口C-a n -&gt; Next，切换到下一个 window C-a p -&gt; Previous，切换到前一个 window C-a 0..9 -&gt; 切换到第 0..9 个 window Ctrl+a [Space] -&gt; 由视窗0循序切换到视窗9 C-a C-a -&gt; 在两个最近使用的 window 间切换 C-a x -&gt; 锁住当前的 window，需用用户密码解锁 C-a d -&gt; detach，暂时离开当前session，将目前的 screen session (可能含有多个 windows) 丢到后台执行，并会回到还没进 screen 时的状态，此时在 screen session 里，每个 window 内运行的 process (无论是前台/后台)都在继续执行，即使 logout 也不影响。 C-a z -&gt; 把当前session放到后台执行，用 shell 的 fg 命令则可回去。 C-a w -&gt; 显示所有窗口列表 C-a t -&gt; time，显示当前时间，和系统的 load C-a k -&gt; kill window，强行关闭当前的 window C-a [ -&gt; 进入 copy mode，在 copy mode 下可以回滚、搜索、复制就像用使用 vi 一样 C-b Backward，PageUp C-f Forward，PageDown H(大写) High，将光标移至左上角 L Low，将光标移至左下角 0 移到行首 $ 行末 w forward one word，以字为单位往前移 b backward one word，以字为单位往后移 Space 第一次按为标记区起点，第二次按为终点 Esc 结束 copy mode C-a ] -&gt; paste，把刚刚在 copy mode 选定的内容贴上]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[密码学学习]]></title>
    <url>%2F3133172261%2F</url>
    <content type="text"><![CDATA[密码学习笔记[toc] 对称加密解密python AES 双向对称加密解密高级加密标准（英语：Advanced Encryption Standard，缩写：AES），在密码学中又称Rijndael加密法，是美国联邦政府采用的一种区块加密标准。这个标准用来替代原先的DES，已经被多方分析且广为全世界所使用。经过五年的甄选流程，高级加密标准由美国国家标准与技术研究院（NIST）于2001年11月26日发布于FIPS PUB 197，并在2002年5月26日成为有效的标准。2006年，高级加密标准已然成为对称密钥加密中最流行的算法之一。 AES加密算法原理 AES密码详解 安装依赖库： 12pip install Cryptopip install binascii 示例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445#!/usr/bin/env python# -*- coding:utf-8 -*- from Crypto.Cipher import AESfrom binascii import b2a_hex, a2b_hex class prpcrypt(): def __init__(self,key): self.key = key self.mode = AES.MODE_CBC #加密函数，如果text不足16位就用空格补足为16位， #如果大于16当时不是16的倍数，那就补足为16的倍数。 def encrypt(self,text): cryptor = AES.new(self.key,self.mode,b&apos;0000000000000000&apos;) #这里密钥key 长度必须为16（AES-128）, #24（AES-192）,或者32 （AES-256）Bytes 长度 #目前AES-128 足够目前使用 length = 16 count = len(text) if count &lt; length: add = (length-count) #\0 backspace text = text + (&apos;\0&apos; * add) elif count &gt; length: add = (length-(count % length)) text = text + (&apos;\0&apos; * add) self.ciphertext = cryptor.encrypt(text) #因为AES加密时候得到的字符串不一定是ascii字符集的，输出到终端或者保存时候可能存在问题 #所以这里统一把加密后的字符串转化为16进制字符串 return b2a_hex(self.ciphertext) #解密后，去掉补足的空格用strip() 去掉 def decrypt(self,text): cryptor = AES.new(self.key,self.mode,b&apos;0000000000000000&apos;) plain_text = cryptor.decrypt(a2b_hex(text)) return plain_text.rstrip(&apos;\0&apos;) if __name__ == &apos;__main__&apos;: pc = prpcrypt(&apos;qwertyuiqwertyui&apos;) #初始化密钥 import sys e = pc.encrypt(sys.argv[1]) #加密 d = pc.decrypt(e) #解密 print &quot;加密:&quot;,e print &quot;解密:&quot;,d 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162#!/usr/bin/env python#coding=utf8 from Crypto.Cipher import AESfrom Crypto import Random # AES根据16位对齐BS = 16 # 转成utf8编码def unicode_to_utf8(s): if isinstance(s, unicode): s = s.encode(&quot;utf-8&quot;) return s # 补充字符,最少1个def pad(s): length = len(s) add = BS - length % BS byte = chr(BS - length % BS) return s + (add * byte) # 去除补充字符def unpad(s): length = len(s) byte = s[length-1:] add = ord(byte) return s[:-add] # classclass AESCipher: #初始化 def __init__(self, key): self.key = key #加密 def encrypt(self, raw): raw = unicode_to_utf8(raw) raw = pad(raw) cipher = AES.new(self.key, AES.MODE_CBC, self.key) return cipher.encrypt(raw) #解密 def decrypt(self, enc): cipher = AES.new(self.key, AES.MODE_CBC, self.key) return unpad(cipher.decrypt(enc)) if __name__ == &apos;__main__&apos;: #注意key是16字节长 key = &quot;f2c85e0140a47415&quot; #初始化 aes = AESCipher(key) s1 = &quot;hello world&quot; en1 = aes.encrypt(s1) de1 = aes.decrypt(en1) print &apos;s1:&apos;, de1]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>密码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SQLAlchemy的使用]]></title>
    <url>%2F1260038886%2F</url>
    <content type="text"><![CDATA[[toc] 使用SQLAlchemyORM技术：Object-Relational Mapping，把关系数据库的表结构映射到对象上。 在Python中，最有名的ORM框架是SQLAlchemy。 pip安装SQLAlchemy： pip install sqlalchemy 首先，导入SQLAlchemy，并初始化DBSession： 123456789101112131415161718192021# 导入:from sqlalchemy import Column, String, create_enginefrom sqlalchemy.orm import sessionmakerfrom sqlalchemy.ext.declarative import declarative_base# 创建对象的基类:Base = declarative_base()# 定义User对象:class User(Base): # 表的名字: __tablename__ = &apos;user&apos; # 表的结构: id = Column(String(20), primary_key=True) name = Column(String(20))# 初始化数据库连接:engine = create_engine(&apos;mysql+mysqlconnector://root:password@localhost:3306/test&apos;)# 创建DBSession类型:DBSession = sessionmaker(bind=engine) 下面，我们看看如何向数据库表中添加一行记录。 由于有了ORM，我们向数据库表中添加一行记录，可以视为添加一个User对象 12345678910# 创建session对象:session = DBSession()# 创建新User对象:new_user = User(id=&apos;5&apos;, name=&apos;Bob&apos;)# 添加到session:session.add(new_user)# 提交即保存到数据库:session.commit()# 关闭session:session.close() SQLAlchemy提供的查询接口如下： 123456789# 创建Session:session = DBSession()# 创建Query查询，filter是where条件，最后调用one()返回唯一行，如果调用all()则返回所有行:user = session.query(User).filter(User.id==&apos;5&apos;).one()# 打印类型和对象的name属性:print(&apos;type:&apos;, type(user))print(&apos;name:&apos;, user.name)# 关闭Session:session.close() ORM框架也可以提供两个对象之间的一对多、多对多等功能。 例如，如果一个User拥有多个Book，就可以定义一对多关系如下： 123456789101112131415class User(Base): __tablename__ = &apos;user&apos; id = Column(String(20), primary_key=True) name = Column(String(20)) # 一对多: books = relationship(&apos;Book&apos;)class Book(Base): __tablename__ = &apos;book&apos; id = Column(String(20), primary_key=True) name = Column(String(20)) # “多”的一方的book表是通过外键关联到user表的: user_id = Column(String(20), ForeignKey(&apos;user.id&apos;)) 当我们查询一个User对象时，该对象的books属性将返回一个包含若干个Book对象的list。 关系定义 一对多: 表示一对多的关系时，在子表类中通过 foreign key (外键)引用父表类。然后，在父表类中通过 relationship() 方法来引用子表的类 1234567891011class Parent(Base): __tablename__ = &apos;parent&apos; id = Column(Integer, primary_key=True) children = relationship(&quot;Child&quot;) # 在父表类中通过 relationship() 方法来引用子表的类集合 class Child(Base): __tablename__ = &apos;child&apos; id = Column(Integer, primary_key=True) parent_id = Column(Integer, ForeignKey(&apos;parent.id&apos;)) # 在子表类中通过 foreign key (外键)引用父表的参考字段 在一对多的关系中建立双向的关系，这样的话在对方看来这就是一个多对一的关系，在子表类中附加一个 relationship() 方法，并且在双方的 relationship() 方法中使用 relationship.back_populates 方法参数： 12345678910111213class Parent(Base): __tablename__ = &apos;parent&apos; id = Column(Integer, primary_key=True) children = relationship(&quot;Child&quot;, back_populates=&quot;parent&quot;) class Child(Base): __tablename__ = &apos;child&apos; id = Column(Integer, primary_key=True) parent_id = Column(Integer, ForeignKey(&apos;parent.id&apos;)) parent = relationship(&quot;Parent&quot;, back_populates=&quot;children&quot;) # 子表类中附加一个 relationship() 方法# 并且在(父)子表类的 relationship() 方法中使用 relationship.back_populates 参数 或者，可以在单一的 relationship() 方法中使用 backref 参数来代替 back_populates 参数 一对一 一对一是两张表之间本质上的双向关系.要做到这一点，只需要在一对多关系基础上的父表中使用 uselist 参数来表示，uselist=False。 多对多多对多关系会在两个类之间增加一个关联的表。这个关联的表在 relationship() 方法中通过 secondary 参数来表示。通常的，这个表会通过 MetaData 对象来与声明基类关联，所以这个 ForeignKey 指令会使用链接来定位到远程的表 1234567891011# 多对多关系中的两个表之间的一个关联表association_table = Table(&apos;association&apos;, Base.metadata, Column(&apos;left_id&apos;, Integer, ForeignKey(&apos;left.id&apos;)), Column(&apos;right_id&apos;, Integer, ForeignKey(&apos;right.id&apos;)))class Parent(Base): __tablename__ = &apos;left&apos; id = Column(Integer, primary_key=True) children = relationship(&quot;Child&quot;,secondary=association_table) # 在父表中的 relationship() 方法传入 secondary 参数，其值为关联表的表名 class Child(Base): __tablename__ = &apos;right&apos; id = Column(Integer, primary_key=True) 指定使用 relationship.back_populates 参数，并且为每一个 relationship() 方法指定共用的关联表： 123456789 association_table = Table(&apos;association&apos;, Base.metadata, Column(&apos;left_id&apos;, Integer, ForeignKey(&apos;left.id&apos;)), Column(&apos;right_id&apos;, Integer, ForeignKey(&apos;right.id&apos;))) class Parent(Base): __tablename__ = &apos;left&apos; id = Column(Integer, primary_key=True) children = relationship(&quot;Child&quot;, secondary=association_table, back_populates=&quot;parents&quot;)class Child(Base): __tablename__ = &apos;right&apos; id = Column(Integer, primary_key=True) parents = relationship(&quot;Parent&quot;, secondary=association_table, back_populates=&quot;children&quot;) 常见的SQLAlchemy列类型.配置选项和关系选项类型名称 | python类型 | 描述—|—|—Integer | int | 常规整形，通常为32位SmallInteger | int | 短整形，通常为16位BigInteger | int/long | 精度不受限整形Float | float | 浮点数Numeric | decimal.Decimal | 定点数String | str | 可变长度字符串Text | str | 可变长度字符串，适合大量文本Unicode | unicode | 浮点数Boolean | bool | 布尔型Date | datetime.date | 日期类型Time | datetime.time | 时间类型Interval | datetime.timedelta | 时间间隔Enum | str|字符列表PickleType | 任意Python对象 | 自动Pickle序列化LargeBinary | str | 二进制 常见的SQLAlchemy列选项可选参数 | 描述—|—primary_key | 如果设置为True，则为该列表的主键unique | 如果设置为True，该列不允许相同值index | 如果设置为True，为该列创建索引，查询效率会更高nullable | 如果设置为True，该列允许为空。如果设置为False，该列不允许空值default | 定义该列的默认值 错误解决当使用migrate upgrade更新数据库时，如果删除以前创建的外键关联字段，会报错： 1553, “Cannot drop index ‘xxx’: needed in a foreign key constraint 解决办法： 进入数据库，手动删除外键所创建的索引： show create table TABLE_NAME; 找到对应的索引，并删除： ALTER TABLE table_name DROP FOREIGN KEY exchange_ibfk_3;]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库简析]]></title>
    <url>%2F3786913082%2F</url>
    <content type="text"><![CDATA[数据库事务的四大特性1. 原子性（Atomicity）&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;原子性是指事务包含的所有操作要么全部成功，要么全部失败回滚，这和前面两篇博客介绍事务的功能是一样的概念，因此事务的操作如果成功就必须要完全应用到数据库，如果操作失败则不能对数据库有任何影响。 2. 一致性（Consistency）&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;一致性是指事务必须使数据库从一个一致性状态变换到另一个一致性状态，也就是说一个事务执行之前和执行之后都必须处于一致性状态。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;拿转账来说，假设用户A和用户B两者的钱加起来一共是5000，那么不管A和B之间如何转账，转几次账，事务结束后两个用户的钱相加起来应该还得是5000，这就是事务的一致性。 3. 隔离性（Isolation）&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;隔离性是当多个用户并发访问数据库时，比如操作同一张表时，数据库为每一个用户开启的事务，不能被其他事务的操作所干扰，多个并发事务之间要相互隔离。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;即要达到这么一种效果：对于任意两个并发的事务T1和T2，在事务T1看来，T2要么在T1开始之前就已经结束，要么在T1结束之后才开始，这样每个事务都感觉不到有其他事务在并发地执行。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;关于事务的隔离性数据库提供了多种隔离级别，稍后会介绍到。 ４. 持久性（Durability）&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;持久性是指一个事务一旦被提交了，那么对数据库中的数据的改变就是永久性的，即便是在数据库系统遇到故障的情况下也不会丢失提交事务的操作。 &emsp;&emsp;例如我们在使用JDBC操作数据库时，在提交事务方法后，提示用户事务操作完成，当我们程序执行完成直到看到提示后，就可以认定事务以及正确提交，即使这时候数据库出现了问题，也必须要将我们的事务完全执行完成，否则就会造成我们看到提示事务处理完毕，但是数据库因为故障而没有执行事务的重大错误。 &emsp;&emsp;以上介绍完事务的四大特性(简称ACID)，现在重点来说明下事务的隔离性，当多个线程都开启事务操作数据库中的数据时，数据库系统要能进行隔离操作，以保证各个线程获取数据的准确性，在介绍数据库提供的各种隔离级别之前，我们先看看如果不考虑事务的隔离性，会发生的几种问题：]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
</search>
